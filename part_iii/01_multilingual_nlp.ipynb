{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with languages other than English\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    " - know how to download and use language models in Stanza, a Python library for processing many languages\n",
    " - how to interface Stanza with the spaCy natural language processing library\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Part II introduced basic natural language processing tasks using examples written in the English language.\n",
    "\n",
    "As a global *lingua franca*, English is a highly-resourced language in terms of natural language processing. Compared to many other languages, the amount of raw and human-annotated data available for English is far greater, while also covering a wider range of domains. In other words, it is easier to find large datasets for English that can be used to train models to perform diverse natural language processing tasks. \n",
    "\n",
    "Unfortunately, the imbalance in resources and research effort has led to a situation where the advances in processing the English language are occasionally claimed to hold for natural language in general. \n",
    "\n",
    "However, as Bender ([2019](https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/)) has shown, *English is not a synonym for natural language*: even if one demonstrates that computers can achieve or surpass human-level performance in some natural language processing task for the English language, this does not mean that one has solved this task or problem for *natural language as a whole*.\n",
    "\n",
    "To measure progress in the field of natural language processing and to ensure that as many languages as possible can benefit from language technology, it is highly desirable to conduct research on processing languages used across the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza – a Python library for processing many languages\n",
    "\n",
    "To get started with working languages other than English, we can use the Stanza library.\n",
    "\n",
    "[Stanza](https://stanfordnlp.github.io/stanza/) is a Python library for natural language processing that provides pre-trained language models for [many languages](https://stanfordnlp.github.io/stanza/available_models.html) (Qi et al. [2020](https://www.aclweb.org/anthology/2020.acl-demos.14/)).\n",
    "\n",
    "Stanza language models are trained on corpora annotated using the [Universal Dependencies](https://universaldependencies.org/) formalism, which means that these models can perform tasks such as tokenization, part-of-speech tagging, morphological tagging and dependency parsing. \n",
    "\n",
    "These are essentially the same tasks that we explored using the spaCy library in [Part II](../part_ii/03_basic_nlp.ipynb).\n",
    "\n",
    "Let's start exploring Stanza by importing the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Stanza library\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process a given language, we must first download a Stanza language model using the `download()` function.\n",
    "\n",
    "The `download()` function requires a single argument, `lang`, which defines the language model to be downloaded.\n",
    "\n",
    "To download a language model for a given language, retrieve the two-letter language code (e.g. *wo*) for the language from [the list of available language models](https://stanfordnlp.github.io/stanza/available_models.html) and pass the language code as a string object to the `lang` argument.\n",
    "\n",
    "For example, the following code would download a model for Wolof, a language spoken in West Africa that belongs to the family of Niger-Congo languages. The model has been trained using the Wolof treebank (Dione [2019](https://www.aclweb.org/anthology/W19-8003/)).\n",
    "\n",
    "```python\n",
    "stanza.download(lang='wo')\n",
    "```\n",
    "\n",
    "For some languages, Stanza provides models that have been trained on different datasets. Within Stanza, these are referred to as *packages*. By default, Stanza automatically downloads the model trained on the largest dataset available for the language in question.\n",
    "\n",
    "To select a model trained on a specific dataset, pass the name of the package as a string object to the `package` argument.\n",
    "\n",
    "To exemplify, the following command downloads a model for Finnish trained on the [*FinnTreeBank*](https://universaldependencies.org/treebanks/fi_ftb/index.html) dataset instead of the default model, which is trained on the [*Turku Dependency Treebank*](https://universaldependencies.org/treebanks/fi_tdt/index.html) dataset.\n",
    "\n",
    "```python\n",
    "stanza.download(lang='fi', package='ftb')\n",
    "```\n",
    "\n",
    "The package names are provided in [the list of language models](https://stanfordnlp.github.io/stanza/available_models.html) available for Stanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "To install the language model into the permanent storage on [CSC Notebooks](https://notebooks.csc.fi/), we must also pass the optional `model_dir` argument that points towards a directory in the permanent storage to the `download()` function.\n",
    "\n",
    "Without using the permanent storage, the models are deleted when the server is shut down.\n",
    "\n",
    "Run the cell below to download the Stanza language model for Wolof into the directory `../stanza_models`.\n",
    "\n",
    "Note that `..` moves up one step in the directory structure relative to this notebook, which places the models into the directory `stanza_models` under the directory `notebooks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Download a Stanza language model for Wolof into the directory \"../stanza_models\"\n",
    "stanza.download(lang=\"wo\", model_dir=\"../stanza_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a language model into Stanza\n",
    "\n",
    "To load a Stanza language model into Python, we must first create a *Pipeline* object using the `Pipeline()` function available in the `stanza` module.\n",
    "\n",
    "To exemplify the procedure, let's initialise a pipeline with a language model for Wolof.\n",
    "\n",
    "To load a language model for Wolof into the pipeline, provide the string `wo` to the `lang` argument.\n",
    "\n",
    "Because we did **not** place the language model into the default directory, we must also provide a string containing the path to the directory with Stanza language models to the `dir` argument.\n",
    "\n",
    "We then store the resulting pipeline under the variable `nlp_wo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### Quick exercise\n",
    "\n",
    "Check [the list of language models](https://stanfordnlp.github.io/stanza/available_models.html) available for Stanza and download a model for a language that you would like to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Write your code below this line and press Shift and Enter to run the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Pipeline() method to initialise a Stanza pipeline with a language model for Wolof, which\n",
    "# is assigned to the variable 'nlp_wo'.\n",
    "nlp_wo = stanza.Pipeline(lang='wo', dir=\"../stanza_models\")\n",
    "\n",
    "# Call the variable to examine the output\n",
    "nlp_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a language model into Stanza returns *Pipeline* object, which consists of a number of *processors* that use the language model to perform various natural language processing tasks.\n",
    "\n",
    "The output above lists the processors under the heading of the same name, together with the packages used to train these processors.\n",
    "\n",
    "As we learned in [Part II](http://localhost:8888/notebooks/part_ii/04_basic_nlp_continued.ipynb#Modifying-spaCy-pipelines), you might not always need all types of linguistic annotations. \n",
    "\n",
    "To speed up processing, you can define the processors to be included in the *Pipeline* object by providing the argument `processors` with a string object that contains the [processor names](https://stanfordnlp.github.io/stanza/pipeline.html#processors) separated by commas.\n",
    "\n",
    "For example, using the following command to create a *Pipeline* would only include the processors for tokenization and part-of-speech tagging.\n",
    "\n",
    "```python\n",
    "nlp_wo = stanza.Pipeline(lang='wo', dir=\"../stanza_models\", processors='tokenize, pos')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text using Stanza\n",
    "\n",
    "Now that we have initialised a Stanza *Pipeline* with a language model, we can feed some text in Wolof to the model under `nlp` as a string object and store the result under the variable `doc_wo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed text to the model under 'nlp_wo'; store result under the variable 'doc'\n",
    "doc_wo = nlp_wo(\"Réew maa ngi lebe turam wi ci dex gi ko peek ci penku ak bëj-gànnaar, te ab balluwaayam bawoo ca Fuuta Jallon ca Ginne, di Dexug Senegaal. Ab kilimaam bu gëwéel la te di bu fendi te yor ñaari jamono: jamonoy nawet (jamonoy taw) ak ju noor (jamonoy fendi).\")\n",
    "\n",
    "# Check the type of the output\n",
    "type(doc_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a Stanza [*Document*](https://stanfordnlp.github.io/stanza/data_objects.html#document) object, which contains the linguistic annotations created by passing the text through the pipeline.\n",
    "\n",
    "To begin with, the attribute `sentences` of a *Document* object contains a list of lists, in which each list stands for a sentence.\n",
    "\n",
    "Let's use the brackets to access the first item `[0]` in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first item in the list of sentences\n",
    "doc_wo.sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the output contains both brackets `[]` and curly braces `{}`, which Python typically uses for lists and dictionaries, respectively, the output is not a list with nested dictionaries, but a Stanza [*Sentence*](https://stanfordnlp.github.io/stanza/data_objects.html#sentence) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the first item in the Document object\n",
    "type(doc_wo.sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Sentence* object contains various attributes and methods for accessing the linguistic annotations created by the language model.\n",
    "\n",
    "If we wish to interact with the annotations using data structures native to Python, we can use the `to_dict()` method to cast the annotations into a list of dictionaries, where each dictionary stands for a single Stanza [*Token*](https://stanfordnlp.github.io/stanza/data_objects.html#token) object.\n",
    "\n",
    "The *key* and *value* pairs in these dictionaries contain the linguistic annotations for each *Token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the first Sentence object into a Python dictionary; store under variable 'doc_dict'\n",
    "doc_dict = doc_wo.sentences[0].to_dict()\n",
    "\n",
    "# Get the dictionary for the first Token\n",
    "doc_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dictionary consists of keys and values, which hold the linguistic annotations.\n",
    "\n",
    "We can retrieve a list of keys available for a Python dictionary using the `keys()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of keys for the first Token in the dictionary 'doc_dict'\n",
    "doc_dict[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the value under the key `lemma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value under key 'lemma' for the first item [0] in the dictionary 'doc_dict'\n",
    "doc_dict[0]['lemma']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the lemma of the word \"réew\", which stands for \"country\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing multiple texts using Stanza\n",
    "\n",
    "To process multiple documents with Stanza, first collect the documents as string objects into a Python list.\n",
    "\n",
    "Let's define a toy example with a couple of example documents in Wolof and store them under the variable `str_docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python list consisting of two strings\n",
    "str_docs = ['Lislaam a ngi njëkk a tàbbi ci Senegaal ci diggante VIIIeelu xarnu ak IXeelu xarnu, ña ko fa dugal di ay yaxantukat yu araab-yu-berber.',\n",
    "            'Li ëpp ci gëstu yi ñu def ci wàllug Gëstu-askan (walla demogaraafi) ci Senegaal dafa sukkandiku ci Waññ (recensement) yi ñu jotoon a def ci 1976, 1988 rawati na 2002.'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of Stanza *Document* objects using a Python list comprehension.\n",
    "\n",
    "These *Document* objects are annotated for their linguistic features when they are passed through a *Pipeline*.\n",
    "\n",
    "At this stage, we simply cast each string in the list `str_docs` to a Stanza *Document* object.\n",
    "\n",
    "Before proceeding to create the *Document* objects, let's examine the list comprehension in greater detail.\n",
    "\n",
    "To begin with, just like lists, list comprehensions are marked by brackets.\n",
    "\n",
    "Next, on the right-hand side of the `for` statement, we use the variable `doc` to refer to items in the list `str_docs` that we are looping over.\n",
    "\n",
    "Now that we can refer to list items using the variable `doc`, we can define what we do to each item on the left-hand side of the `for` statement.\n",
    "\n",
    "For each item in the list `str_docs`, we initialise an empty `Document` object and pass two inputs to the newly created object: \n",
    "\n",
    " 1. an empty list `[]` that will eventually hold the linguistic annotations, \n",
    " 2. the contents of the string variable under `doc` to the argument `text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list comprehension to create a Python list with Stanza Document objects.\n",
    "docs_wo_in = [stanza.Document([], text=doc) for doc in str_docs]\n",
    "\n",
    "# Call the variable to check the output\n",
    "docs_wo_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't let the output fool you: what looks like two empty Python lists nested within another list are actually Stanza *Document* objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_wo_out = nlp_wo(docs_wo_in)\n",
    "\n",
    "docs_wo_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing Stanza with spaCy\n",
    "\n",
    "If you are more familiar with the spaCy library for natural language processing, you can also use some of the Stanza language models in spaCy!\n",
    "\n",
    "This can be achieved using a Python library named [spacy-stanza](https://spacy.io/universe/project/spacy-stanza), which interfaces the two libraries.\n",
    "\n",
    "Given that Stanza currently has more pre-trained language models available than spaCy, the spacy-stanza library considerably increases the number of languages available for spaCy.\n",
    "\n",
    "There is, however, **one major limitation**: the language model in Stanza must be [supported by spaCy](https://spacy.io/usage/models#languages).\n",
    "\n",
    "For example, we cannot use the Stanza language model for Wolof in spaCy, because spaCy does not support the Wolof language.\n",
    "\n",
    "To start using Stanza language models in spaCy, let's start by importing the spacy-stanza library (`spacy_stanza`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy and spacy-stanza libraries\n",
    "import spacy\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Now that we have imported both spaCy and spacy-stanza libraries into Python, let's download a Stanza language model for Finnish into the same directory where we placed the language model for Wolof above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a Stanza language model for Finnish into the directory \"../stanza_models\"\n",
    "stanza.download(lang=\"fi\", model_dir=\"../stanza_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy supports [the Finnish language](https://spacy.io/usage/models#languages), we can load the Stanza language model for Finnish into spaCy using the spacy-stanza library.\n",
    "\n",
    "This can be achieved using the `load_pipeline()` function available in spacy-stanza.\n",
    "\n",
    "This function takes two arguments: the argument `name` requires a string object that defines the two-letter code for the Stanza language model to be loaded, while `dir` takes a string object that points to the directory with Stanza language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the load_pipeline function to load a Stanza model into spaCy.\n",
    "# Assign the result under the variable 'nlp'.\n",
    "nlp_fi = spacy_stanza.load_pipeline(name=\"fi\", dir=\"../stanza_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce the same kind of output as loading a Stanza model.\n",
    "\n",
    "If we examine the resulting object under the variable `nlp_fi` using Python's `type()` function, we will see that the object is indeed a spaCy *Language* object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the object under 'nlp_fi'\n",
    "type(nlp_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, this object behaves just like any other spaCy *Language* object that we learned to use in [Part II](../part_ii/03_basic_nlp.ipynb#Performing-basic-NLP-tasks-using-spaCy).\n",
    "\n",
    "We can explore the use of this model by processing a few sentences from a recent [news article](https://yle.fi/aihe/artikkeli/2021/03/08/yleiso-aanesti-tarja-halonen-on-inspiroivin-nainen-karkikolmikkoon-ylsivat-myos) in written Finnish.\n",
    "\n",
    "We feed the text as a string object to the *Language* object under `nlp_fi` and store the result under the variable `doc_fi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the text to the language model under 'nlp_fi', store result under 'doc_fi'\n",
    "doc_fi = nlp_fi('Tove Jansson keräsi 148 ääntä eli 18,2% annetuista äänistä. Kirjailija, kuvataiteilija ja pilapiirtäjä tuli kansainvälisesti tunnetuksi satukirjoistaan ja sarjakuvistaan.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by retrieve a list of sentences from the *Doc* object, which are available under the attribute `sents`.\n",
    "\n",
    "Remember that the object under the `sents` attribute is a Python generator that yields *Doc* objects. To examine them, we must catch the objects into a suitable data structure, which in this case is a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentences contained in the Doc object 'doc_fi'.\n",
    "# Cast the result into list.\n",
    "sents_fi = list(doc_fi.sents)\n",
    "\n",
    "# Call the variable to check the output\n",
    "sents_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, for example, to use the `render()` function in the `displacy` submodule of spaCy to visualise the syntactic dependencies for the first sentence under `sents_fi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the displacy submodule\n",
    "from spacy import displacy\n",
    "\n",
    "# Use the render function to render the first item [0] in the list 'sents_fi'.\n",
    "# Pass the argument 'style' with the value 'dep' to visualise syntactic dependencies.\n",
    "displacy.render(sents_fi[0], style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy will raise a warning about storing custom attributes when writing the *Doc* object to disk for visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_fi:\n",
    "    \n",
    "    print(token, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(doc_fi.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_fi(\"kansainvälisesti\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
