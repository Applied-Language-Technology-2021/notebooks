{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings in spaCy\n",
    "\n",
    "The previous [section](../part_iii/03_embeddings.ipynb) introduced the concept of word embeddings using a toy example with just a few sentences.\n",
    "\n",
    "This section focuses on word embeddings learned from massive volumes of text and how to use them in spaCy.\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    " - understand what word embeddings can be used for\n",
    " - know how to use word embeddings in spaCy\n",
    " - know how to visualise words in their embedding space\n",
    " - know how to use contextual word embeddings in spaCy\n",
    " - know how to add a custom component to the spaCy pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings in spaCy\n",
    "\n",
    "spaCy provides 300-dimensional word embeddings for several languages, which have been learned from large corpora.\n",
    "\n",
    "In other words, each word in the model's vocabulary is represented by a list of 300 floating point numbers – a vector – and these vectors are embedded into a 300-dimensional space.\n",
    "\n",
    "To explore the use of word vectors in spaCy, let's start by loading a large language model for English, which contains word vectors for 685 000 *Tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Load a large language model and assign it to the variable 'nlp'\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an example sentence and feed it to the language model under `nlp` for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example sentence\n",
    "text = \"The Shiba Inu is a dog that is more like a cat.\"\n",
    "\n",
    "# Feed example sentence to the language model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the word vector for the second *Token* in the *Doc* object (\"Shiba\"), which can be accessed through the *Token* attribute `vector`.\n",
    "\n",
    "Instead of printing the 300 floating point numbers that constitute the vector, let's limit the output to the first thirty dimensions using `[:30]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the second Token in the Doc object, and the first\n",
    "# 30 dimensions of its vector representation\n",
    "doc[1].vector[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These floating point numbers encode information that the model has learned for the *Token*, based on the linguistic context in which the *Token* occurs.\n",
    "\n",
    "Just as in the [previous section](../part_iii/03_embeddings.ipynb), we can use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to measure the similarity of two vectors. \n",
    "\n",
    "spaCy implements cosine similarity in the `similarity()` method, which is included for *Tokens*, *Spans* and *Docs*.\n",
    "\n",
    "The `similarity()` method can take any of these objects as input and calculate cosine similarity between the objects.\n",
    "\n",
    "For convenience, let's assign the *Tokens* 'dog' and 'cat' in our example *Doc* object into the variables `dog` and `cat` and compare their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assing the fifth and eleventh items in the Doc into their own variables\n",
    "dog = doc[5]\n",
    "cat = doc[11]\n",
    "\n",
    "# Compare the similarity between Tokens 'dog' and 'cat'\n",
    "dog.similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the vectors for cats and dogs are very similar, because these words are likely to appear in similar linguistic contexts, as both cats and dogs are common household pets.\n",
    "\n",
    "For comparison, let's retrieve the vector representation for a snake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the string \"snake\" to the language model; store result under 'snake'\n",
    "snake = nlp(\"snake\")\n",
    "\n",
    "# Compare the similarity of 'snake' and 'dog'\n",
    "snake.similarity(dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the vector for snake is not that similar to the vector for dog.\n",
    "\n",
    "Finally, let's compare the similarity of the vectors for car and snake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake.similarity(nlp(\"car\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the vectors for car and snake are not very similar at all, as these words are not likely to occur in similar linguistic contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "### Quick exercise\n",
    "\n",
    "Define two words with similar or dissimilar meanings, feed them to the language model and compare their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Write your code below this line and press Shift and Enter to run the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*spaCy* also provides word vectors for entire *Doc* objects or *Span* objects within them.\n",
    "\n",
    "To move beyond *Tokens*, let's start by examining the *Doc* object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the variable to examine the output\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector for this *Doc* object is also available under the attribute `vector`.\n",
    "\n",
    "Instead of examining the actual vector stored under `vector`, let's retrieve its `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'shape' attribute for the vector\n",
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the *Token* objects above, the *Doc* object has a 300-dimensional vector.\n",
    "\n",
    "The vector representation for the entire *Doc* is calculated by **averaging the vectors** for each *Token* in the *Doc*.\n",
    "\n",
    "The same applies to _Spans_, which can be examined by retrieving the noun phrases in `doc`, which are available under the `noun_chunks` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the noun chunks under the attribute 'noun_chunks'. This returns\n",
    "# a generator, so we cast the output into a list named 'n_chunks'.\n",
    "n_chunks = list(doc.noun_chunks)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example sentence has three noun phrases. \n",
    "\n",
    "Let's examine the shape of the vector for the first noun phrase, \"The Shiba Inu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the vector for the first noun chunk in the list\n",
    "n_chunks[0].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the *Doc* object, the *Span* object has a 300-dimensional vector. This vector is also calculated by averaging the vectors for each *Token* in the *Span*.\n",
    "\n",
    "We can also use the `similarity()` method to measure cosine similarity between Spans.\n",
    "\n",
    "Let's compare the similarity of noun phrases \"The Shiba Inu\" `[0]` and \"a dog\" `[1]`.\n",
    "\n",
    "Based on our world knowledge, we should know that both noun phrases belong to the same semantic field: as a dog breed, the Shiba Inu is a hyponym of dog. For this reason, they should presumably occur in similar contexts and thus their vectors should be close to each other in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the similarity of the two noun chunks\n",
    "n_chunks[0].similarity(n_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the embeddings for the noun phrases \"The Shiba Inu\" and \"a dog\" are about as similar as those of a dog and a snake above!\n",
    "\n",
    "To understand why the vectors for these noun phrases are dissimilar, we must delve deeper into the word embeddings and the effects of averaging vectors for linguistic units that are larger than a single *Token*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising word embeddings\n",
    "\n",
    "whatlies is an open source library for visualising \"what lies\" in word embeddings, that is, what kinds of information they encode (Warmerdam et al. [2020](https://www.aclweb.org/anthology/2020.nlposs-1.8.pdf)).\n",
    "\n",
    "The whatlies library is intended to support the interpretation of high-dimensional word embeddings. In this context, high-dimensional refers to the number of dimensions in the embedding space.\n",
    "\n",
    "High-dimensional spaces are notoriously difficult to comprehend, as our experience as embodied beings is strongly grounded into a three-dimensional space complemented by the dimension of time.\n",
    "\n",
    "Visualisations such as those provided by whatlies may help to alleviate this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the whatlies library\n",
    "import whatlies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whatlies provides wrappers for language models from various popular natural language processing libraries, including spaCy.\n",
    "\n",
    "These wrappers are essentially Python classes that know what to do with a language model from a given Python library.\n",
    "\n",
    "We therefore import the `SpacyLanguage` object from whatlies and *wrap* the spaCy language model stored under the `nlp` variable into this object. \n",
    "\n",
    "We then assign the result to the variable `language_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wrapper class for spaCy language models\n",
    "from whatlies.language import SpacyLanguage\n",
    "\n",
    "# Wrap the spaCy language model under 'nlp' into the\n",
    "# whatlies SpacyLanguage class and assign the result \n",
    "# under the variable 'language_model'\n",
    "language_model = SpacyLanguage(nlp)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a *SpacyLanguage* object that wraps a spaCy language model named `nlp`.\n",
    "\n",
    "Let's continue by taking a closer look at the list of noun phrases stored under `n_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each noun phrase\n",
    "for chunk in n_chunks:\n",
    "    \n",
    "    # Loop over each Token in noun phrase\n",
    "    for token in chunk:\n",
    "        \n",
    "        # Print Token attributes 'text', 'oov', 'vector' and separate\n",
    "        # each attribute by a string object containing a tabulator \\t\n",
    "        # sequence for pretty output\n",
    "        print(token.text, '\\t', token.is_oov, '\\t', token.vector[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `is_oov` attribute of a *Token* corresponds to *out of vocabulary* and returns `True` or `False` depending on whether the *Token* is included in the vocabulary of the language model or not.\n",
    "\n",
    "In this case, all *Tokens* are present in the vocabulary, hence their value is `False`.\n",
    "\n",
    "`vector[:3]` returns the first three dimensions in the 300-dimensional word vector.\n",
    "\n",
    "If a *Token* were out of vocabulary, the values of each dimension would be set to zero.\n",
    "\n",
    "Let's examine this by mistyping \"Shiba Inu\" as \"shibainu\", feed this string to the language model under `nlp` and retrieve the values for the first three dimensions of its vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the string 'shibainu' to the language model and assign\n",
    "# the result under the variable 'shibainu'\n",
    "shibainu = nlp(\"shibainu\")\n",
    "\n",
    "# Retrieve the first three dimensions of its word vector\n",
    "shibainu.vector[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three dimensions are set to zero, suggesting that the word is out of vocabulary.\n",
    "\n",
    "We can easily double-check this using the `is_oov` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shibainu[0].is_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important, because the values of each dimension determine the *direction* and *magnitude* of the vector in the embedding space.\n",
    "\n",
    "Zero values provide no information about the direction and magnitude of the vector.\n",
    "\n",
    "We can use the visualisations in whatlies to explore this idea further.\n",
    "\n",
    "To examine the embeddings for noun phrases in the `n_chunks` list using whatlies, we must populate the list with string objects rather than *Spans*.\n",
    "\n",
    "We therefore define a list comprehension that retrieves the plain text stored under the attribute `text` of a *Span* object and stores this string into a list of the same name, that is, `n_chunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over noun chunks, retrieve plain text and store\n",
    "# the result under the variable 'n_chunks'\n",
    "n_chunks = [n_chunk.text for n_chunk in n_chunks]\n",
    "\n",
    "# Call the variable to examine the output\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the noun chunks in a list, we can feed them to the *SpacyLanguage* object stored under `language_model`.\n",
    "\n",
    "The input must be placed in brackets `[ ]` right after the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve embeddings for items in list 'n_chunks'\n",
    "# and store the result under 'embeddings'\n",
    "embeddings = language_model[n_chunks]\n",
    "\n",
    "# Call the variable to examine the output\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an *EmbSet* object which stores the embeddings for our noun phrases.\n",
    "\n",
    "To visualize the embeddings, we can use the `plot()` method of the *EmbSet* object.\n",
    "\n",
    "The arguments `kind`, `color`, `x_axis` and `y_axis` instruct *whatlies* to draw red arrows that plot the direction and magnitude of each vector along dimensions 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.plot(kind='arrow', color='red', x_axis=1, y_axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector originates at the point $(0, 0)$. We can see that along dimensions 1 and 2, the directions and magnitudes of the vectors differ considerably.\n",
    "\n",
    "This results from averaging the vectors for each *Token* in the noun phrase.\n",
    "\n",
    "To understand the effects of averaging vectors, let's retrieve embeddings for the indefinite article \"a\", the noun \"dog\" and the noun phrase \"a dog\" and plot the result along the same dimensions as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model[['a', 'dog', 'a dog']].plot(kind='arrow', color='red', x_axis=1, y_axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector for \"a dog\" is positioned right in the middle between the vectors for \"a\" and \"dog\", because the vector for \"a dog\" is an average of the vectors for \"a\" and \"dog\".\n",
    "\n",
    "This raises the question whether averaging vectors for individual *Tokens* is suitable for representing linguistic units beyond single words, because the direction and magnitude of a vector are supposed to represent the \"meaning\" of a word in relation to those of other words in the model vocabulary.\n",
    "\n",
    "Averaging *Token* vectors for representing clauses and sentences may dilute the information encoded in the vectors. Another issue emerges from the relative weight of the averaged weights: whether the indefinite article \"a\" and the noun \"dog\" are equally important is questionable.\n",
    "\n",
    "Here the problem is that word embeddings are used to learn representations for a *single Token* based on their linguistic context of occurrence, but the resulting representations **do not** encode any information about the context in which the *Token* appears.\n",
    "\n",
    "Put differently, word embeddings leverage information about linguistic context during learning, but this information is not encoded into the word embeddings themselves.\n",
    "\n",
    "This limitation has been addressed in the [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), an alternative architecture for learning representations for words.\n",
    "\n",
    "Some models that build on this architecture include BERT (Devlin et al. [2019](https://www.aclweb.org/anthology/N19-1423/)) and GPT-3 (Brown et al. [2020](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)). Both models are massive, featuring billions of parameters that encode information about words *and* the contexts in which they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual embeddings from Transformers\n",
    "\n",
    "One core idea behind Transformers is to learn a language model from massive amounts of data and to adapt what the model has learned to more specific tasks. Put differently, the idea is to transfer what the model has learned to other tasks.\n",
    "\n",
    "spaCy provides Transformer-based language models for English and several other languages, which outperform the \"traditional\" pipelines in terms of accuracy.\n",
    "\n",
    "Let's start by loading a Transformer-based for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Transformer-based language model; assing to variable 'nlp'\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the surface, a *Language* object that contains a Transformer-based model looks and works just like any other language model in spaCy.\n",
    "\n",
    "However, if we look under the hood of the *Language* object using the `pipeline` attribute, we will see that the first component in the processing pipeline is a *Transformer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'pipeline' attribute to examine the processing pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer component generates vector representations that are then used for making predictions about the *Doc* and the *Tokens* contained within.\n",
    "\n",
    "These include, among others, the standard linguistic annotations in the form of part-of-speech tags, syntactic dependencies, named entities and lemmas.\n",
    "\n",
    "Let's define an example sentence and feed it to the Transformer-based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed an example sentence to the model; store output under 'example_doc'\n",
    "example_doc = nlp(\"Helsinki is the capital of Finland.\")\n",
    "\n",
    "# Check the length of the Doc object\n",
    "example_doc.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy stores the vector representations generated by the Transformer into a *TransformerData* object, which can be accessed under the custom attribute `trf_data` of a *Doc* object.\n",
    "\n",
    "Remember that spaCy stores custom attributes under a dummy attribute marked by an underscore `_`, which is reserved for user-defined attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the 'trf_data' object\n",
    "type(example_doc._.trf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Transformer is contained in the *TransformerData* object.\n",
    "\n",
    "To begin with, the `tensors` attribute contains a list with vector representations generated by the Transformer for both *Tokens* and the entire *Doc*.\n",
    "\n",
    "The first item in the `tensors` list contains the output for individual *Tokens*, whereas the second item holds the output for the entire *Doc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the first tensor in the list\n",
    "example_doc._.trf_data.tensors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer output is stored in a *tensor*, which is a mathematical term for describing a bundle of numerical objects and their shape. In this case, we have a batch of 1 that consists of 11 vectors with 768 dimensions each.\n",
    "\n",
    "We can access the first ten dimensions of each vector using the expression `[:10]`. Note that we need the preceding expression `[0]` to enter the first \"batch\" of vectors in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first ten dimensions of the tensor\n",
    "example_doc._.trf_data.tensors[0][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike word embeddings, which leverage contextual information to *learn* representations for tokens, these embeddings also encode information about the context of occurrence into the representation!\n",
    "\n",
    "But why is a *Doc* with 7 *Tokens* represented by 11 vectors?\n",
    "\n",
    "In the [previous section](../part_iii/03_embeddings.ipynb) we learned that model size grows together with vocabulary. Because Transformers are trained on massive volumes of text, the size of the model's vocabulary must be limited.\n",
    "\n",
    "Instead of learning a mapping between individual words and vectors, Transformers use advanced tokenizers that identify frequently occurring character sequences in the data and learn embeddings for these sequences instead.\n",
    "\n",
    "Let's examine how the example *Doc* under `example_doc` was tokenized for the Transformer.\n",
    "\n",
    "This information is stored under the `input_texts` key under the `tokens` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc._.trf_data.tokens['input_texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lists the tokens provided to the Transformer *by its own tokenizer*. In other words, the Transformer does not use the same tokens as spaCy.\n",
    "\n",
    "The input begins and terminates with tokens `<s>` and `</s>`, which mark the beginning and the end of the input sequence. The Transformer tokenizer also uses the character `Ġ` as a prefix to indicate that the token is preceded by a whitespace character.\n",
    "\n",
    "For the most part, the Transformer tokens correspond roughly to those produced by spaCy, except for \"Helsinki\".\n",
    "\n",
    "Because the token \"Helsinki\" is not present in the Transformer's vocabulary, the token is broken down into three character sequences that exist in the vocabulary: `H`, `els` and `inki`. Their vectors are used to construct a representation for the token \"Helsinki\".\n",
    "\n",
    "To map these vectors to *Tokens* in the spaCy *Doc* object, we must retrieve alignment information from the `align` attribute of the *TransformerData* object.\n",
    "\n",
    "The `align` attribute can be indexed using *Token* indices of the *Doc* object. To exemplify, we can retrieve the first *Token* \"Helsinki\" in the *Doc* object `doc` using the expression `example_doc[0]`.\n",
    "\n",
    "We then use this index to retrieve alignment data from a *Ragged* object, which is stored under the `align` attribute. More specifically, we need the information stored under its attribute `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first spaCy Token, \"Helsinki\", and its alignment data\n",
    "example_doc[0], example_doc._.trf_data.align[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` attribute contains a NumPy array that identifies which tensors under `tensors` contain representations for this *Token*. In this case, vectors 1, 2 and 3 in the batch of 11 vectors contain the representation for \"Helsinki\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the contextual embeddings from the Transformer efficiently, we can define a component that retrieves contextual embeddings for *Docs*, *Spans* and *Tokens* and add this component to the spaCy pipeline.\n",
    "\n",
    "This can be achieved by creating a new Python *Class* – a user-defined object with attributes and methods.\n",
    "\n",
    "Because the new *Class* will become a component of the spaCy pipeline, we must first import the *Language* object and let spaCy know that we are now defining a new pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Language object under the 'language' module in spaCy,\n",
    "# and NumPy for calculating cosine similarity.\n",
    "from spacy.language import Language\n",
    "import numpy as np\n",
    "\n",
    "# We use the @ character to register the following Class definition\n",
    "# with spaCy under the name 'tensor2attr'.\n",
    "@Language.factory('tensor2attr')\n",
    "\n",
    "# We begin by declaring the class name: Tensor2Attr. The name is \n",
    "# declared using 'class', followed by the name and a colon.\n",
    "class Tensor2Attr:\n",
    "    \n",
    "    # We continue by defining the first method of the class, \n",
    "    # __init__(), which is called when this class is used for \n",
    "    # creating a Python object. Custom components in spaCy \n",
    "    # require passing two variables to the __init__() method:\n",
    "    # 'name' and 'nlp'. The variable 'self' refers to any\n",
    "    # object created using this class!\n",
    "    def __init__(self, name, nlp):\n",
    "        \n",
    "        # We do not really do anything with this class, so we\n",
    "        # simply move on using 'pass' when the object is created.\n",
    "        pass\n",
    "\n",
    "    # The __call__() method is called whenever some other object\n",
    "    # is passed to an object representing this class. Since we know\n",
    "    # that the class is a part of the spaCy pipeline, we already know\n",
    "    # that it will receive Doc objects from the preceding layers.\n",
    "    # We use the variable 'doc' to refer to any object received.\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        # When an object is received, the class will instantly pass\n",
    "        # the object forward to the 'add_attributes' method. The\n",
    "        # reference to self informs Python that the method belongs\n",
    "        # to this class.\n",
    "        self.add_attributes(doc)\n",
    "        \n",
    "        # After the 'add_attributes' method finishes, the __call__\n",
    "        # method returns the object.\n",
    "        return doc\n",
    "    \n",
    "    # Next, we define the 'add_attributes' method that will modify\n",
    "    # the incoming Doc object by calling a series of methods.\n",
    "    def add_attributes(self, doc):\n",
    "        \n",
    "        # spaCy Doc objects have an attribute named 'user_hooks',\n",
    "        # which allows customising the default attributes of a \n",
    "        # Doc object, such as 'vector'. We use the 'user_hooks'\n",
    "        # attribute to replace the attribute 'vector' with the \n",
    "        # Transformer output, which is retrieved using the \n",
    "        # 'doc_tensor' method defined below.\n",
    "        doc.user_hooks['vector'] = self.doc_tensor\n",
    "        \n",
    "        # We then perform the same for both Spans and Tokens that\n",
    "        # are contained within the Doc object.\n",
    "        doc.user_span_hooks['vector'] = self.span_tensor\n",
    "        doc.user_token_hooks['vector'] = self.token_tensor\n",
    "        \n",
    "        # We also replace the 'similarity' method, because the \n",
    "        # default 'similarity' method looks at the default 'vector'\n",
    "        # attribute, which is empty! We must first replace the\n",
    "        # vectors using the 'user_hooks' attribute.\n",
    "        doc.user_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_span_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_token_hooks['similarity'] = self.get_similarity\n",
    "    \n",
    "    # Define a method that takes a Doc object as input and returns \n",
    "    # Transformer output for the entire Doc.\n",
    "    def doc_tensor(self, doc):\n",
    "        \n",
    "        # Return Transformer output for the entire Doc. As noted\n",
    "        # above, this is the last item under the attribute 'tensor'.\n",
    "        # Use [0] to access the batch.\n",
    "        return doc._.trf_data.tensors[-1][0]\n",
    "    \n",
    "    # Define a method that takes a Span as input and returns the Transformer \n",
    "    # output.\n",
    "    def span_tensor(self, span):\n",
    "        \n",
    "        # Get alignment information for Span. This is achieved by using\n",
    "        # the 'doc' attribute of Span that refers to the Doc that contains\n",
    "        # this Span. We then use the 'start' and 'end' attributes of a Span\n",
    "        # to retrieve the alignment information. Finally, we flatten the\n",
    "        # resulting array to use it for indexing.\n",
    "        tensor_ix = span.doc._.trf_data.align[span.start: span.end].data.flatten()\n",
    "        \n",
    "        # Get Token tensors under tensors[0]; the second [0] accesses batch\n",
    "        tensor = span.doc._.trf_data.tensors[0][0][tensor_ix]\n",
    "        \n",
    "        # Sum vectors along axis 0 (columns). This yields a 768-dimensional\n",
    "        # vector for each spaCy Token.\n",
    "        return tensor.sum(axis=0)\n",
    "    \n",
    "    # Define a function that takes a Token as input and returns the Transformer\n",
    "    # output.\n",
    "    def token_tensor(self, token):\n",
    "        \n",
    "        # Get alignment information for Token; flatten array for indexing.\n",
    "        # Again, we use the 'doc' attribute of a Token to get the parent Doc,\n",
    "        # which contains the Transformer output.\n",
    "        tensor_ix = token.doc._.trf_data.align[token.i].data.flatten()\n",
    "        \n",
    "        # Get Token tensors under tensors[0]; the second [0] accesses batch\n",
    "        tensor = token.doc._.trf_data.tensors[0][0][tensor_ix]\n",
    "\n",
    "        # Sum vectors along axis 0 (columns). This yields a 768-dimensional\n",
    "        # vector for each spaCy Token.\n",
    "        return tensor.sum(axis=0)\n",
    "    \n",
    "    # Define a function for calculating cosine similarity between vectors\n",
    "    def get_similarity(self, doc1, doc2):\n",
    "        \n",
    "        # Calculate and return cosine similarity\n",
    "        return np.dot(doc1.vector, doc2.vector) / (doc1.vector_norm * doc2.vector_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the previous cell is relatively long, note that the comments explaining the actions taken took up most of the space!\n",
    "\n",
    "With the *Class* `Tensor2Attr` defined, we can now add it to the pipeline by referring to the name we registered with spaCy using `@Language.factory()`, that is, `tensor2attr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the component named 'tensor2attr', which we registered using the\n",
    "# @Language decorator and its 'factory' method to the pipeline.\n",
    "nlp.add_pipe('tensor2attr')\n",
    "\n",
    "# Call the 'pipeline' attribute to examine the pipeline\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the component named `tensor2attr` was added to the spaCy pipeline.\n",
    "\n",
    "This component stores the Transformer-based contextual embeddings for *Docs*, *Spans* and *Tokens* under the `vector` attribute.\n",
    "\n",
    "Let's explore contextual embeddings by defining two *Doc* objects and feeding them to the Transformer-based language model under `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two example sentences and process them using the Transformer-based\n",
    "# language model under 'nlp'.\n",
    "doc_city = nlp(\"Helsinki is the capital of Finland.\")\n",
    "doc_money = nlp(\"The company is close to bankruptcy because its capital is gone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noun \"capital\" has two different meanings in these sentences: in `doc_city`, \"capital\" refers to a city, whereas in `doc_money` the word refers to money.\n",
    "\n",
    "The Transformer should have learned this difference and encode this information into the resulting vector based on the context in which the word occurs.\n",
    "\n",
    "Let's fetch the *Token* corresponding to \"capital\" in each example and retrieve their vector representations under the `vector` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vectors for the two Tokens corresponding to \"capital\";\n",
    "# assign to variables 'city' and 'money'.\n",
    "city = doc_city[3]\n",
    "money = doc_money[8]\n",
    "\n",
    "# Compare the similarity of the two meanings of 'capital'\n",
    "city.similarity(money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the vectors for the word \"capital\" are not very similar, because the Transformer also encodes information about their context of occurrence into the vectors, which has allowed it to learn that the same linguistic form may have different meanings in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "### Quick exercise\n",
    "\n",
    "Define two words with similar forms but different meanings, feed them to the Transformer-based language model under `nlp` and compare their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Write your code below this line and press Shift and Enter to run the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should have given you a basic understanding of word embeddings and their use in spaCy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
