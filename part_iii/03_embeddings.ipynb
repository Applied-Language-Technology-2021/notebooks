{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "> ⚠️ The cells of this notebook have been executed to facilitate the use of [readthedocs.io](applied-language-technology.readthedocs.io/). If you wish to work through the notebook step-by-step, go to the *Kernel* menu and select *Restart & Clear Output*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a technique for learning numerical representations for individual words based on the context in which the words appear. \n",
    "\n",
    "Such numerical representations are a prerequisite for computational processing, that is, performing calculations, statistical analyses and making predictions.\n",
    "\n",
    "After reading this section, you should:\n",
    "\n",
    " - understand the relation between word embeddings and the underlying distributional hypothesis\n",
    " - how different aspects of linguistic structure can be represented numerically\n",
    " - understand how word embeddings are learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: linguistic perspectives on word embeddings\n",
    "\n",
    "The inspiration for word embeddings is often attributed to the following observation by the English linguist [J.R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth):\n",
    "\n",
    "> \"You shall know a word by the company it keeps.\"\n",
    "\n",
    "What Firth means is that the meaning of a word can be inferred by examining that word in its context of occurrence.\n",
    "\n",
    "Martin ([2016](https://doi.org/10.1080/00437956.2016.1141939): 45) notes that Firth's interest in the co-occurrences of words, or *collocations*, spawned a wealth of linguistic research, particularly in the field of corpus linguistics. \n",
    "\n",
    "Firth's observation also resonates with the so-called *distributional hypothesis* proposed by Harris ([1954](https://doi.org/10.1080/00437956.1954.11659520)), which posits that linguistic items with similar distributions should have similar meanings.\n",
    "\n",
    "Examining the distributional hypothesis from a linguistic perspective, Magnus Sahlgren ([2008](http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf): 33–34) points out that:\n",
    "\n",
    "> \"Proponents of distributional methods often seem comfortable to ascribe meaning to distributional representations without explaining what they _mean_ by meaning. For the non-distributionalist, on the other hand, this will surely seem odd if not completely outrageous, since meaning is usually taken to involve both reference to objects and situations in the world outside language, and to concepts and ideas inside the mind of the language user.\"\n",
    "\n",
    "Sahlgren ([2008](http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf): 34) argues that the distributional hypothesis has roots in \"structuralist soil\", that is, [Ferdinand de Saussure](https://en.wikipedia.org/wiki/Ferdinand_de_Saussure)'s ideas about the structure of language.\n",
    "\n",
    "Saussure described language from two perspectives: *langue*, the abstract system constituted by language, and *parole*, particular instances of language produced by the underlying system of *langue*. \n",
    "\n",
    "Saussure characterised *langue* as having *paradigmatic* and *syntagmatic* axes of organisation, which allow making choices between alternatives and combining them into larger units. The available choices emerge through oppositions: alternatives can only be identified by what they are and what they are not.\n",
    "\n",
    "![](img/parasyn.svg)\n",
    "\n",
    "Saussure's and Firth's ideas were taken further by [M.A.K. Halliday](https://en.wikipedia.org/wiki/Michael_Halliday), who incorporated them into the foundations of a theory of language known as [systemic functional linguistics](https://en.wikipedia.org/wiki/Systemic_functional_linguistics) (for a recent overview of the field, see Martin [2016](https://doi.org/10.1080/00437956.2016.1141939)). \n",
    "\n",
    "In contrast to Saussure's view of language as a static system of oppositions, Halliday emphasises the role of *choice* for language. \n",
    "\n",
    "Halliday argues that language is defined by *meaning potential*, which is realised *dynamically* by making choices within intersecting linguistic systems. These systems are provided by *lexicogrammar*, which Halliday describes as a cline: the choices made within language become increasingly delicate when moving from grammar to lexis (see e.g. Fontaine [2017](https://doi.org/10.1186/s40554-017-0051-7)).\n",
    "\n",
    "Against this backdrop, we will now turn to explore the distributional hypothesis from both syntagmatic and paradigmatic perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the distributional hypothesis\n",
    "\n",
    "### A syntagmatic perspective\n",
    "\n",
    "As introduced above, syntagmatic organisations emerge as choices made within language are combined into larger units.\n",
    "\n",
    "To explore syntagmatic organisations in a corpus of texts, we can build a co-occurrence matrix to examine which words co-occur with each other within some sequence. These sequences may be motivated linguistically, as exemplified by a clause or a sentence, or they may simply consist of words within some distance of one another. \n",
    "\n",
    "To do so, we must retrieve unique words and count their occurrences across all sequences.\n",
    "\n",
    "Let's start by importing the *spaCy* library and a medium-sized language model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Load a medium-sized language model for English; assign to variable 'nlp'\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a toy example consisting of example sentences in a list, which we assign to the variable `sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Helsinki is the capital of Finland', 'Tallinn is the capital of Estonia', 'The two capitals are joined by a ferry connection', 'Travelling between Helsinki and Tallinn takes about two hours', 'Ferries depart from downtown Helsinki and Tallinn']\n"
     ]
    }
   ],
   "source": [
    "# Create list\n",
    "sents = [\"Helsinki is the capital of Finland\",\n",
    "         \"Tallinn is the capital of Estonia\",\n",
    "         \"The two capitals are joined by a ferry connection\",\n",
    "         \"Travelling between Helsinki and Tallinn takes about two hours\",\n",
    "         \"Ferries depart from downtown Helsinki and Tallinn\"]\n",
    "\n",
    "# Print list contents\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then feed this list to the English language model under `nlp` and store the resulting _Doc_ object under the variable `docs`.\n",
    "\n",
    "To process the example sentences effectively, we can use the `pipe()` method, which takes a list as an input.\n",
    "\n",
    "The `pipe()` method returns a generator object, which we have to cast into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Helsinki is the capital of Finland,\n",
       " Tallinn is the capital of Estonia,\n",
       " The two capitals are joined by a ferry connection,\n",
       " Travelling between Helsinki and Tallinn takes about two hours,\n",
       " Ferries depart from downtown Helsinki and Tallinn]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed the list of example sentences 'sents' to the pipe() method.\n",
    "# Cast the result into a list and store under the variable 'docs'.\n",
    "docs = list(nlp.pipe(sents))\n",
    "\n",
    "# Call the variable to check the output\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of convenience and simplicity, we examine the co-occurrence of lemmas rather than the inflected forms of words. \n",
    "\n",
    "To count the lemmas in each sentence, we must import the `LEMMA` object from spaCy's `attrs` module. \n",
    "\n",
    "`LEMMA` is just an object that refers to this particular linguistic feature, which we can pass to the `count_by()` method of a *Doc* object to instruct *spaCy* to count these linguistic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LEMMA object from the 'attrs' module of spaCy\n",
    "from spacy.attrs import LEMMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a Python *dictionary comprehension* to count the lemmas in each sentence.\n",
    "\n",
    "A dictionary comprehension is declared using curly braces `{ }`, which are also used to designate a dictionary in Python.\n",
    "\n",
    "Because Python dictionaries consist of *keys* and *values*, we need **two** items to populate the new `lemma_counts` dictionary using a dictionary comprehension:\n",
    "\n",
    " 1. `i` refers to the number returned by the `enumerate` function that keeps count of items\n",
    " 2. `doc` refers to the current document in `docs`, our _list_ of _Doc_ objects\n",
    " \n",
    "Note that on the right-hand side of the `for` statement, these two variables are separated by a comma.\n",
    " \n",
    "The left-hand side of the `for` statement defines what is actually stored in the `lemma_counts` dictionary. \n",
    "\n",
    "In this case, we store the count `i` as the *key* and assign the output of the `count_by` method as the *value*.\n",
    "\n",
    "On the left-hand side, these variables are separated by a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {332692160570289739: 1,\n",
       "  10382539506755952630: 1,\n",
       "  7425985699627899538: 1,\n",
       "  15481038060779608540: 1,\n",
       "  886050111519832510: 1,\n",
       "  4881666681900411319: 1},\n",
       " 1: {7392857733388117912: 1,\n",
       "  10382539506755952630: 1,\n",
       "  7425985699627899538: 1,\n",
       "  15481038060779608540: 1,\n",
       "  886050111519832510: 1,\n",
       "  15428882767191480669: 1},\n",
       " 2: {7425985699627899538: 1,\n",
       "  11711838292424000352: 1,\n",
       "  15481038060779608540: 1,\n",
       "  10382539506755952630: 1,\n",
       "  16238441731120403936: 1,\n",
       "  16764210730586636600: 1,\n",
       "  11901859001352538922: 1,\n",
       "  16008623592554433546: 1,\n",
       "  14753437861310164020: 1},\n",
       " 3: {9016120516514741834: 1,\n",
       "  7508752285157982505: 1,\n",
       "  332692160570289739: 1,\n",
       "  2283656566040971221: 1,\n",
       "  7392857733388117912: 1,\n",
       "  6789454535283781228: 1,\n",
       "  942632335873952620: 1,\n",
       "  11711838292424000352: 1,\n",
       "  9748623380567160636: 1},\n",
       " 4: {16008623592554433546: 1,\n",
       "  11568774473013387390: 1,\n",
       "  7831658034963690409: 1,\n",
       "  18137549281339502438: 1,\n",
       "  332692160570289739: 1,\n",
       "  2283656566040971221: 1,\n",
       "  7392857733388117912: 1}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a dictionary comprehension to populate the 'lemma_counts' dictionary\n",
    "lemma_counts = {i: doc.count_by(LEMMA) for i, doc in enumerate(docs)}\n",
    "\n",
    "# Call the variable to check the output\n",
    "lemma_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `lemma_counts` dictionary contains the _Doc_ numbers as *keys* and dictionaries as *values*!\n",
    "\n",
    "At a glance, these dictionaries obviously report some counts, but as we learned in [Part II](../part_ii/03_basic_nlp.ipynb), *spaCy* uses hash values for *Tokens* for efficiency, which explains the presence of weird sequences of numbers.\n",
    "\n",
    "We can map the hash values to actual words by performing a slightly more complicated dictionary comprehension.\n",
    "\n",
    "Below we update the `lemma_counts` dictionary in two steps:\n",
    "\n",
    " 1. Looping over *key* (`i`) and *value* (`counter`) pairs, which are accessible through the `items()` method of the `lemma_counts` dictionary.\n",
    " \n",
    " This is done by the part on the **right** hand side of the `for` statement, e.g. `i, counter in lemma_counts.items()`.\n",
    " \n",
    " \n",
    " 2. Updating the *keys* and *values* of the `lemma_counts` dictionary by preserving the original key `i` that defines the sentence number.\n",
    " \n",
    " Because the dictionary *value* stored under `counter` is another dictionary, we must define yet another dictionary comprehension!\n",
    " \n",
    " This dictionary comprehension is just like the one above, but this time we update the *keys* of the `counter` dictionary to replace the hash values with text.\n",
    " \n",
    " To do so, we access the sentence in question from the list of spaCy *Doc* objects `docs` by referring to the sentence number under `i`. We then provide the hash value under `k` to the `vocab` attribute to fetch the contents of the attribute `text`.\n",
    " \n",
    "This illustrates just how much a single line of Python can achieve using powerful expressions such as dictionary comprehensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Helsinki': 1, 'be': 1, 'the': 1, 'capital': 1, 'of': 1, 'Finland': 1},\n",
       " 1: {'Tallinn': 1, 'be': 1, 'the': 1, 'capital': 1, 'of': 1, 'Estonia': 1},\n",
       " 2: {'the': 1,\n",
       "  'two': 1,\n",
       "  'capital': 1,\n",
       "  'be': 1,\n",
       "  'join': 1,\n",
       "  'by': 1,\n",
       "  'a': 1,\n",
       "  'ferry': 1,\n",
       "  'connection': 1},\n",
       " 3: {'travel': 1,\n",
       "  'between': 1,\n",
       "  'Helsinki': 1,\n",
       "  'and': 1,\n",
       "  'Tallinn': 1,\n",
       "  'take': 1,\n",
       "  'about': 1,\n",
       "  'two': 1,\n",
       "  'hour': 1},\n",
       " 4: {'ferry': 1,\n",
       "  'depart': 1,\n",
       "  'from': 1,\n",
       "  'downtown': 1,\n",
       "  'Helsinki': 1,\n",
       "  'and': 1,\n",
       "  'Tallinn': 1}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a dictionary comprehension to replace the keys of the 'lemma_counts' dictionary\n",
    "lemma_counts = {i: {docs[i].vocab[k].text: v for k, v in counter.items()} for i, counter in lemma_counts.items()}\n",
    "\n",
    "# Call the variable to check the output\n",
    "lemma_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a dictionary with *Doc* numbers as *keys* and dictionaries with lemma counts as *values*.\n",
    "\n",
    "To better understand these counts, we can organise them into a tabular form using a *pandas* DataFrame.\n",
    "\n",
    "This table provides a co-occurrence matrix of lemmas and sentences.\n",
    "\n",
    "We can easily populate the DataFrame by feeding the dictionary of *Docs* and lemma count dictionaries to the `from_dict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estonia</th>\n",
       "      <th>Finland</th>\n",
       "      <th>Helsinki</th>\n",
       "      <th>Tallinn</th>\n",
       "      <th>a</th>\n",
       "      <th>about</th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>between</th>\n",
       "      <th>by</th>\n",
       "      <th>...</th>\n",
       "      <th>downtown</th>\n",
       "      <th>ferry</th>\n",
       "      <th>from</th>\n",
       "      <th>hour</th>\n",
       "      <th>join</th>\n",
       "      <th>of</th>\n",
       "      <th>take</th>\n",
       "      <th>the</th>\n",
       "      <th>travel</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Estonia  Finland  Helsinki  Tallinn    a  about  and   be  between   by  \\\n",
       "0      0.0      1.0       1.0      0.0  0.0    0.0  0.0  1.0      0.0  0.0   \n",
       "1      1.0      0.0       0.0      1.0  0.0    0.0  0.0  1.0      0.0  0.0   \n",
       "2      0.0      0.0       0.0      0.0  1.0    0.0  0.0  1.0      0.0  1.0   \n",
       "3      0.0      0.0       1.0      1.0  0.0    1.0  1.0  0.0      1.0  0.0   \n",
       "4      0.0      0.0       1.0      1.0  0.0    0.0  1.0  0.0      0.0  0.0   \n",
       "\n",
       "   ...  downtown  ferry  from  hour  join   of  take  the  travel  two  \n",
       "0  ...       0.0    0.0   0.0   0.0   0.0  1.0   0.0  1.0     0.0  0.0  \n",
       "1  ...       0.0    0.0   0.0   0.0   0.0  1.0   0.0  1.0     0.0  0.0  \n",
       "2  ...       0.0    1.0   0.0   0.0   1.0  0.0   0.0  1.0     0.0  1.0  \n",
       "3  ...       0.0    0.0   0.0   1.0   0.0  0.0   1.0  0.0     1.0  1.0  \n",
       "4  ...       1.0    1.0   1.0   0.0   0.0  0.0   0.0  0.0     0.0  0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# We then create a pandas DataFrame using the .from_dict() method,\n",
    "# to which we pass the dictionary under 'lemma_counts'. We then\n",
    "# sort the index in an ascending order using the sort_index() method.\n",
    "df = pd.DataFrame.from_dict(lemma_counts).sort_index(ascending=True)\n",
    "\n",
    "# Replace NaN values with zeroes using the fillna() method.\n",
    "# Finally, we use .T attribute to transpose the DataFrame.\n",
    "# This switches the place of columns and rows to improve legibility.\n",
    "df = df.fillna(0).T\n",
    "\n",
    "# Print out the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a DataFrame with *unique lemmas* across all sentences in the columns, while the individual *Docs* occupy rows at indices 0–4.\n",
    "\n",
    "Each cell in the DataFrame counts how many times a given lemma occurs in a *Doc*.\n",
    "\n",
    "We can use these counts to examine the co-occurrence of lemmas within each *Doc*.\n",
    "\n",
    "Let's examine the values for the first sentence by using the `iloc` accessor, which allows accessing the indices in a *pandas* DataFrame.\n",
    "\n",
    "We access the first *Doc* at index `0` and retrieve the values using the `values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an *array*, or a list of values that counts the occurrence of lemmas in the *Doc*.\n",
    "\n",
    "In mathematics, such lists of numbers are called *vectors*.\n",
    "\n",
    "The length of this vector is defined by the number of *unique* lemmas across *all* *Docs*.\n",
    "\n",
    "This number defines the *dimensionality* of the vector.\n",
    "\n",
    "Because each sentence is now represented by a numerical vector, we can easily perform mathematical operations, such as calculate the distance between each pair of vectors to evaluate their similarity.\n",
    "\n",
    "To do so, we can import the `cosine_similarity()` function from the *scikit-learn* library, which allows measuring [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.66666667, 0.40824829, 0.13608276, 0.15430335],\n",
       "       [0.66666667, 1.        , 0.40824829, 0.13608276, 0.15430335],\n",
       "       [0.40824829, 0.40824829, 1.        , 0.11111111, 0.12598816],\n",
       "       [0.13608276, 0.13608276, 0.11111111, 1.        , 0.37796447],\n",
       "       [0.15430335, 0.15430335, 0.12598816, 0.37796447, 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the function for measuring cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Evaluate cosine similarity between vectors\n",
    "sim = cosine_similarity(df.values)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a 5 by 5 matrix with measures for cosine similarity between each pair of sentences.\n",
    "\n",
    "To help us interpret this table, let's import the `heatmap()` function from the _seaborn_ library to visualise the cosine similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the heatmap function from the seaborn library\n",
    "from seaborn import heatmap\n",
    "\n",
    "# Provide the cosine similarity matrix under 'sim' to the heatmap() function\n",
    "heatmap(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row and column in the heatmap represent a single *Doc*.\n",
    "\n",
    "The heatmap shows a diagonal line across the table with values of 1.0, because each *Doc* is also compared to itself!\n",
    "\n",
    "These *Docs* are naturally perfectly similar, which results in a value of 1.0.\n",
    "\n",
    "What we can also see is that *Docs* 0 and 1 feature some similarities, while most *Docs* are not similar at all, as their values are 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, a co-occurrence matrix can describe *syntagmatic structures* by paradigmatic choices that co-occur within some fixed unit or window of a given length (in our case, a sentence).\n",
    "\n",
    "The downside to this approach, however, is that when the size of the vocabulary in the corpus increases, so does the size of the co-occurrence matrix. For each new word, we must add another dimension for keeping track of its occurrences.\n",
    "\n",
    "Furthermore, the co-occurrence matrix does not contain information about the *order* in which the words appear. For this reason, such representations are often characterised using the term \"[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)\".\n",
    "\n",
    "The lack of information about word sequences complicates the identification of paradigmatic alternatives, that is, words that *could* have occurred in connection with other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A paradigmatic perspective\n",
    "\n",
    "To explore the distributional hypothesis from a paradigmatic perspective, we must shift the target of description from sequences of text to words.\n",
    "\n",
    "Just as we represented entire sentences using vectors with the help of a co-occurrence matrix, we can do the same for individual words.\n",
    "\n",
    "Let's start by taking the unique lemmas in our corpus of *Docs*.\n",
    "\n",
    "These can be easily retrieved from the DataFrame `df` by accessing the `columns` attribute, because the columns correspond to the unique lemmas. We then use the `tolist()` method to convert the output into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Estonia',\n",
       " 'Finland',\n",
       " 'Helsinki',\n",
       " 'Tallinn',\n",
       " 'a',\n",
       " 'about',\n",
       " 'and',\n",
       " 'be',\n",
       " 'between',\n",
       " 'by',\n",
       " 'capital',\n",
       " 'connection',\n",
       " 'depart',\n",
       " 'downtown',\n",
       " 'ferry',\n",
       " 'from',\n",
       " 'hour',\n",
       " 'join',\n",
       " 'of',\n",
       " 'take',\n",
       " 'the',\n",
       " 'travel',\n",
       " 'two']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the unique lemmas from the DataFrame and convert to list\n",
    "unique_lemmas = df.columns.tolist()\n",
    "\n",
    "# Call the variable to examine the output\n",
    "unique_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a technique called *one-hot encoding* to represent each lemma numerically.\n",
    "\n",
    "One-hot encoding maps each lemma to a vector, which consists of zeroes except at one one dimension, where the value is one. This encodes the identity of a given word.\n",
    "\n",
    "Let's examine this in practice by mapping each lemma to a corresponding one-hot vector.\n",
    "\n",
    "To do so, we import *NumPy*, a Python library for working with arrays (Harris et al. [2020](https://doi.org/10.1038/s41586-020-2649-2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Estonia\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Finland\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Helsinki\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Tallinn\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] a\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] about\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] and\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] be\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] between\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] capital\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] connection\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] depart\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] downtown\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] ferry\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] from\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] hour\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] join\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] of\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] take\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] the\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] travel\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] two\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy, assign name 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Define an empty placeholder dictionary for lemmas and their vectors\n",
    "lemma_vectors = {}\n",
    "\n",
    "# Loop over the list of unique lemmas; count items using enumerate()\n",
    "for i, lemma in enumerate(unique_lemmas):\n",
    "    \n",
    "    # Create a vector with a length that corresponds to that of the\n",
    "    # 'unique_lemmas' list. This matches the size of our vocabulary.\n",
    "    # The np.zeros() function fills this vector with zeroes.\n",
    "    vector = np.zeros(shape=len(unique_lemmas))\n",
    "    \n",
    "    # Use the brackets to access the vector at the current index 'i',\n",
    "    # which we retrieve during the loop over the list 'unique_lemmas'.\n",
    "    # Set the value to one instead of zero at this position in the vector.\n",
    "    vector[i] = 1\n",
    "    \n",
    "    # Store the lemma and vector into the dictionary\n",
    "    lemma_vectors[lemma] = vector\n",
    "    \n",
    "    # Print out the vector and the lemma it corresponds to\n",
    "    print(vector, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, dimensions that have a value of 1.0 form a diagonal line across the vectors.\n",
    "\n",
    "This diagonal line emerges because each dimension of the vector encodes the presence of a unique lemma. \n",
    "\n",
    "We can use these vectors to encode sequences of words into numerical representations, which allows us to capture paradigmatic choices in their syntagmatic context of occurrence!\n",
    "\n",
    "Put differently, we can now observe which words occur in similar contexts and are potentially *paradigmatic alternatives*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Helsinki\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] be\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] the\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] capital\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] of\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Finland\n"
     ]
    }
   ],
   "source": [
    "# Loop over Tokens in the first Doc in the list\n",
    "for token in docs[0]:\n",
    "    \n",
    "    # Get the lemma of each token under the attribute 'lemma_'\n",
    "    # and use this as a key for the 'lemma_vectors' dictionary\n",
    "    # to retrieve the associated vector. Then print the lemma\n",
    "    # itself.\n",
    "    print(lemma_vectors[token.lemma_], token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can join these vector representations of individual words into a *matrix*, a multidimensional \"table\" of vectors.\n",
    "\n",
    "We can use NumPy's `vstack()` method to stack the vectors vertically, that is, place the vectors on top of each other to form a matrix that represents the entire *Doc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a list comprehension to collect the vectors for each lemma in the first Doc\n",
    "sentence_matrix = np.vstack([lemma_vectors[token.lemma_] for token in docs[0]])\n",
    "\n",
    "# Call the variable to examine the output\n",
    "sentence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the shape of the resulting matrix using the attribute `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 23)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` attribute contains the shape of the matrix: six rows, one for each lemma, and 23 columns, which encode the presence of individual lemmas in the vocabulary.\n",
    "\n",
    "A matrix allows representing any sequence of *Tokens* numerically. However, as the size of the vocabulary grows, so does the number of dimensions needed to represent each unique word in the vocabulary.\n",
    "\n",
    "These vectors are often characterised as *sparse*, because most dimensions do not encode any information, but consist of zero values.\n",
    "\n",
    "To make word embeddings efficient, each dimension of a vector should encode some information about the word and the context in which it occurs.\n",
    "\n",
    "This can be achieved by learning word embeddings directly from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning word embeddings\n",
    "\n",
    "As introduced above, we can now map each lemma to a sparse vector, which consists of zeros except in one position (or dimension), which encodes the identity of a given lemma by having a value of 1.\n",
    "\n",
    "We can use the mappings between a word and its vector to encode any sequence of words, provided that the words are included in our vocabulary.\n",
    "\n",
    "Because we can now represent any word numerically as a vector, we can use these vectors to train an algorithm to predict which words are likely to occur close to each other.\n",
    "\n",
    "To do so, we must determine what we consider as a neighbour to a given word.\n",
    "\n",
    "To exemplify, let's choose the second *Doc* in the list `docs` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tallinn is the capital of Estonia"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the second Doc in the list: remember zero-indexing!\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the two neighbouring words on either side of a word count as its neighbours.\n",
    "\n",
    "To examine a single word, let's access the third _Token_ in the second *Doc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the second Doc and the third Token\n",
    "docs[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `nbor()` method of a *Token* to fetch the neighbours of the current *Token*.\n",
    "\n",
    "The `nbor()` method takes an integer as input, which determines the position relative to the current *Token*. \n",
    "\n",
    "Negative integers refer to the indices of *Tokens* that come before the current *Token*, while positive integers refer to those that come after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the preceding Token at position -1\n",
    "docs[1][2].nbor(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capital"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the follow Token at position +1\n",
    "docs[1][2].nbor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an algorithm to predict neighbouring words, we must first collect all words that occur within a two-word window of a given word.\n",
    "\n",
    "In linguistics, words that co-occur with each other are known as a collocation.\n",
    "\n",
    "Note that our approach only approximates collocations, since our definition of a word's neighbourhood is not motivated linguistically, whereas actual collocates often are. In other words, our search for collocates can easily violate the borders of perceived linguistic structures.\n",
    "\n",
    "Furthermore, not all words have neighbours on both sides: the words that start or finish a sentence or some other sequence will not have preceding or following words.\n",
    "\n",
    "To deal with this problem, we can use the `try` and `except` statements in Python to catch the errors arising from missing neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to hold Tokens and their neighbouring Tokens\n",
    "pairs = []\n",
    "\n",
    "# Loop over each Doc in the list of Docs under the variable 'docs'\n",
    "for doc in docs:\n",
    "    \n",
    "    # Loop over each Token in a Doc\n",
    "    for token in doc:\n",
    "        \n",
    "        # Loop over the indices of neighbouring Tokens that are of \n",
    "        # interest to us.\n",
    "        for neighbour_i in [-2, -1, 1, 2]:\n",
    "        \n",
    "            # Try to retrieve neighbour at position 'neighbour_i'\n",
    "            try:\n",
    "\n",
    "                # Assign the preceding Token into the variable 'context'\n",
    "                context = token.nbor(neighbour_i)\n",
    "            \n",
    "                # Append a tuple consisting of the current Token\n",
    "                # and the neighbouring Token to the list 'pairs'\n",
    "                pairs.append((token.lemma_, context.lemma_))\n",
    "        \n",
    "            # Use the except command to catch the error arising if \n",
    "            # there is no preceding Token ('IndexError')\n",
    "            except IndexError:\n",
    "\n",
    "                # Move to the next Token in the list of neighbours\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a list of tuples that contain word pairs that occur within two words of each other.\n",
    "\n",
    "Let's print out the first 10 pairs to examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Helsinki', 'be'),\n",
       " ('Helsinki', 'the'),\n",
       " ('be', 'Helsinki'),\n",
       " ('be', 'the'),\n",
       " ('be', 'capital'),\n",
       " ('the', 'Helsinki'),\n",
       " ('the', 'be'),\n",
       " ('the', 'capital'),\n",
       " ('the', 'of'),\n",
       " ('capital', 'be')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first 10 tuples in the list 'pairs'\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word in each tuple can be described as the *target* lemma, whereas the second constitutes a *context* lemma.\n",
    "\n",
    "To set the stage for making predictions, we must collect all target words in the list and their matching context words, while also converting them into their one-hot encoded numerical representations. \n",
    "\n",
    "This can be achieved by a list comprehension, which loops over the tuples in the list `pairs`. We can then fetch the one-hot encoded vector from the dictionary `lemma_vectors` by using the target lemma as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 23)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list comprehension that collects target lemmas and stores\n",
    "# the one-hot encoded vectors into a list named 'targets'\n",
    "targets = [lemma_vectors[target] for target, context in pairs]\n",
    "\n",
    "# Stack all the target lemmas into a matrix\n",
    "targets = np.vstack(targets)\n",
    "\n",
    "# Call the variable to check the size of the matrix\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the `shape` attribute reveals that we have a total of 118 target lemmas. We then repeat the same operation for their matching context lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 23)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a list comprehension that collects context lemmas and stores\n",
    "# the one-hot encoded vectors into a list named 'context'\n",
    "context = [lemma_vectors[context] for target, context in pairs]\n",
    "\n",
    "# Stack all the context lemmas into a matrix\n",
    "context = np.vstack(context)\n",
    "\n",
    "# Call the variable to check the size of the matrix\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Each target lemma in `targets` now has a matching context lemma under `context`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a small neural network, which attempts to learn a mapping between the target and context lemmas.\n",
    "\n",
    "In other words, given a target word, the network attempts to predict which context lemmas are likely to occur near the target lemma. \n",
    "\n",
    "To do so, we define a small neural network using [Keras](https://keras.io/).\n",
    "\n",
    "More specifically, we implement a variant of an algorithm called *Word2vec* (for *word to vector*), proposed by Tomas Mikolov et al. ([2013](https://arxiv.org/abs/1301.3781v3)).\n",
    "\n",
    "Keras is a part of the TensorFlow deep learning library; we import both Keras and *Dense*, a specific type of neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining an *Input* layer, which receives the one-hot encoded vectors stored under the variable `targets`.\n",
    "\n",
    "We fetch the size of the second dimension (`[1]`) of the `targets` matrix available under the `shape` attribute. We then provide this information to the `shape` *argument* of the *Input* layer, which informs the neural network about the size of the incoming data.\n",
    "\n",
    "We also use the `name` argument to name the layer as `input_layer`. \n",
    "\n",
    "We store the resulting *Input* object under the variable `network_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = keras.Input(shape=targets.shape[1], name='input_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a *Dense* layer with two neurons, as defined by the `units` argument. \n",
    "\n",
    "We connect the *Input* layer to this layer by placing the variable `network_input` in parentheses after the *Dense* layer, e.g. `Dense(...)(network_input)`. \n",
    "\n",
    "We assign the output of the *Dense* layer under variable `hidden_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = Dense(units=2, activation=None, name='hidden_layer')(network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions about which context lemmas occur close to the target lemmas, we then define another *Dense* layer whose `units` argument matches the size of our vocabulary.\n",
    "\n",
    "This layer acts as the output layer of our network.\n",
    "\n",
    "By setting the `activation` argument to `softmax`, the network will return probabilities for all lemmas in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(units=context.shape[1], activation='softmax', name='output_layer')(hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then combine the layers defined above into a network and assign the result under the variable `embedding_model`.\n",
    "\n",
    "This is achieved using the *Model* object in Keras and its two arguments: `inputs` and `outputs`, to which we must provide the input and output layers of our network, that is, `input_layer` and `output_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = keras.Model(inputs=network_input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the `compile()` method to compile the model and define a loss function using the `loss` argument. \n",
    "\n",
    "The purpose of a loss function is to measure the error between predicted and actual context lemmas. This error is used to adjust the neurons in a way that should potentially improve the predictions next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.compile(loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the resulting model, we call the `summary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 23)]              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 2)                 48        \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 23)                69        \n",
      "=================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now compiled a simple neural network. The network has a single hidden layer with two neurons, which acts as a bottleneck for information.\n",
    "\n",
    "To learn how to predict the context lemmas based on the target lemmas, the model must learn to condense information contained in the sparse input vectors.\n",
    "\n",
    "The next step is to train the model. This is achieved using the model's `fit()` function.\n",
    "\n",
    "This function requires defining several arguments. The arguments `x` and `y` correspond to the inputs and outputs. These consist of the target and context lemmas, which are stored in the matrices under `targets` and `context`, respectively. \n",
    "\n",
    "We examine 64 pairs of target and context lemmas at the same time, as defined by the keyword `batch`, and loop over all pairs of lemmas 1500 times, as defined by the keyword `epochs`.\n",
    "\n",
    "We also provide the `verbose` argument with a value of 0 to avoid flooding the notebook with status messages about the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18db912b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a model to the data\n",
    "embedding_model.fit(x=targets,\n",
    "                    y=context,\n",
    "                    batch_size=64,\n",
    "                    epochs=1500,\n",
    "                    verbose=0\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the neural network has been trained, we can use the model to predict which context lemmas are likely to occur close to the target lemma.\n",
    "\n",
    "Let's examine these predictions by retrieving the one-hot encoded vector for the lemma \"be\" and input this to the model.\n",
    "\n",
    "To do so, we must use the `expand_dims()` function from NumPy to add a dummy axis in front of the network, because our network expects to receive vectors in batches. This tells the network that the input consists of a single vector.\n",
    "\n",
    "We store the input under the variable `input_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 23)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a dummy axis to the input vector\n",
    "input_array = np.expand_dims(lemma_vectors['be'], axis=0)\n",
    "\n",
    "# Check the shape of the input array\n",
    "input_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then input the vector to the model using the `predict()` method, which returns an array of probabilities.\n",
    "\n",
    "We store these probabilities under the variable `prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03895581, 0.04121093, 0.04212597, 0.03517739, 0.0285058 ,\n",
       "        0.01165018, 0.01420426, 0.13894387, 0.00280192, 0.04856807,\n",
       "        0.17241693, 0.01035706, 0.00747751, 0.00536414, 0.01841469,\n",
       "        0.01281205, 0.01841119, 0.07494045, 0.06392696, 0.02405063,\n",
       "        0.13948733, 0.00773188, 0.04246505]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed the input to the neural network; store predictions under 'prediction'\n",
    "prediction = embedding_model.predict(input_array)\n",
    "\n",
    "# Call the variable to examine the output\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities correspond to the unique lemmas in our vocabulary.\n",
    "\n",
    "To examine which lemma is the most likely to occur in the neighbourhood of the lemma \"be\", we can use NumPy's `argmax()` method to find which dimension in the `prediction` vector has the highest value.\n",
    "\n",
    "This gives us an integer, which we can use as an index for the list of lemmas under `unique_lemmas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'capital'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_lemmas[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, predicting the most likely neighbour of a given lemma is not the actual *goal* of the training procedure, but this task is more like a proxy for the true objective: to learn numerical representations for individual words. \n",
    "\n",
    "To predict context lemmas, the network must learn useful representations for target lemmas. One may think of these numerical representations as the identity of the word, which was previously encoded using a one-hot vector.\n",
    "\n",
    "These representations are learned by the hidden layer of the neural network, which contains two neurons. \n",
    "\n",
    "These neurons have *parameters*, commonly referred to as *weights*, which are adjusted as the network learns to improve predictions based on the error estimated by the loss function.\n",
    "\n",
    "The weights of a model can be retrieved using the `get_weights()` method of a Keras *Model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve model weights; assign the resulting list to 'model_weights'\n",
    "model_weights = embedding_model.get_weights()\n",
    "\n",
    "# The weights of the hidden layer are the first item in the list\n",
    "hidden_layer_weights = model_weights[0]\n",
    "\n",
    "# Call the variable and use the 'shape' attribute to examine size\n",
    "hidden_layer_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the hidden layer consist of 23 two-dimensional vectors, one two-dimensional vector for each unique lemma in the vocabulary.\n",
    "\n",
    "Let's print out the first five vectors to examine their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.2699678 ,  1.7514169 ],\n",
       "       [ 2.6050782 ,  2.012647  ],\n",
       "       [-0.05873575, -1.18698   ],\n",
       "       [ 0.6555827 , -1.1613712 ],\n",
       "       [-1.206744  ,  2.0521815 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the first five items in the weight matrix\n",
    "hidden_layer_weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the sparse one-hot encoded vectors, the representations learned by the hidden layer may be characterised as *dense*, as each dimension of the vector encodes some information.\n",
    "\n",
    "We can think of these two-dimensional values as coordinates and plot these dimensions against each other to examine position of each lemma in the two-dimensional space.\n",
    "\n",
    "This is the *embedding space* for the vectors.\n",
    "\n",
    "To visualise the embedding space and the vectors within this space, we use a dictionary comprehension to map each unique lemma to its two-dimensional representation in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect hidden layer weights into a dictionary using a dictionary comprehension\n",
    "lemma_embeddings = {lemma: hidden_layer_weights[i] for i, lemma in enumerate(unique_lemmas)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the *matplotlib* library and create a figure. The `dpi` argument sets the resolution of the figure to 150 dots per inch.\n",
    "\n",
    "We then loop over the lemmas and their vector representations in the `lemma_embeddings` dictionary. \n",
    "\n",
    "The `items()` method of a dictionary returns both keys and values, which we then add to the *matplotlib* figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import the pyplot component from matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a resolution of 200 DPI\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "# Loop over the key/value pairs in the dictionary\n",
    "for lemma, coordinates in lemma_embeddings.items():\n",
    "\n",
    "    # Unpack the coordinates variable into two variables\n",
    "    # These stand for the horizontal/vertical coordinates\n",
    "    x, y = coordinates[0], coordinates[1]\n",
    "    \n",
    "    # Use the scatter() method to add the x and y coordinates\n",
    "    # to the figure\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    # Use the annotate() method to add the lemmas as labels \n",
    "    # to the coordinate pairs, which must be wrapped into a tuple\n",
    "    plt.annotate(lemma, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the two dimensions of the vectors against each other provides a visualisation of the vectors in the two-dimensional embedding space.\n",
    "\n",
    "The vector representations of lemmas that occur in similar contexts should be positioned close to each other in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section should have given you a basic understanding of the distributional hypothesis, which has been influential in developing the concept of word embeddings.\n",
    "\n",
    "At the same time, you should have learned that word embeddings are not magic. They are learned directly from the data using a proxy task such as predicting the neighbouring word.\n",
    "\n",
    "However, one must understand that the toy example presented above merely scratches the surface of language modelling. \n",
    "\n",
    "Contemporary approaches use models with complex architectures and billions of parameters, which attempt to encode more information about the neighbouring words, in order to distinguish between homonymic forms such as \"bank\" as a financial institution and \"bank\" as an area close to the river.\n",
    "\n",
    "In the following [section](../part_iii/04_embeddings_continued.ipynb), we dive deeper into word embeddings and how they can be used in spaCy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
